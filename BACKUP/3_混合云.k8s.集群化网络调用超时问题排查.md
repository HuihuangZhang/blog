# [混合云 k8s 集群化网络调用超时问题排查](https://github.com/HuihuangZhang/blog/issues/3)

# 1. 背景
dev 环境的 k8s 集群有 3 台机器：
- 192.168.1.130 (dev-master)
  - Control-plane，master 节点
  - 云A
- 192.168.1.131 (dev-worker)
  - Worker
  - 云A
  - ip link show
```
ip link show
1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN mode DEFAULT group default qlen 1000
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
2: eth0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1400 qdisc mq state UP mode DEFAULT group default qlen 1000
    link/ether 00:16:3e:08:66:3b brd ff:ff:ff:ff:ff:ff
    altname enp3s1
3: flannel.1: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1350 qdisc noqueue state UNKNOWN mode DEFAULT group default 
    link/ether c2:ea:76:57:bb:78 brd ff:ff:ff:ff:ff:ff
4: cni0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1300 qdisc noqueue state UP mode DEFAULT group default qlen 1000
    link/ether 0a:53:89:c9:b0:42 brd ff:ff:ff:ff:ff:ff
11: veth55968cc8@if2: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1450 qdisc noqueue master cni0 state UP mode DEFAULT group default 
    link/ether 52:01:07:58:83:0a brd ff:ff:ff:ff:ff:ff link-netns cni-d860bed9-8aad-8c0a-0813-0928059a2aa6
16: veth0651c946@if2: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1300 qdisc noqueue master cni0 state UP mode DEFAULT group default 
    link/ether ea:89:5a:e9:7f:0e brd ff:ff:ff:ff:ff:ff link-netns cni-dcb74613-9f00-2ee0-62a2-04e9c27e5728
29: veth6430dd7e@if2: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1300 qdisc noqueue master cni0 state UP mode DEFAULT group default 
    link/ether 9a:36:d7:57:09:ac brd ff:ff:ff:ff:ff:ff link-netns cni-73ab49ce-3dc2-e4e7-ce37-2a91a3ddd6a7
31: vethfbbfb963@if2: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1300 qdisc noqueue master cni0 state UP mode DEFAULT group default 
    link/ether 16:d9:d8:eb:0c:93 brd ff:ff:ff:ff:ff:ff link-netns cni-9cc35d1c-f966-d4a0-4cc3-62586b689905
32: veth5c020cdd@if2: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1300 qdisc noqueue master cni0 state UP mode DEFAULT group default 
    link/ether 8a:a1:54:7c:3e:ea brd ff:ff:ff:ff:ff:ff link-netns cni-341deb3c-605d-9688-384d-db0d1001c861
34: veth0762e11a@if2: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1300 qdisc noqueue master cni0 state UP mode DEFAULT group default 
    link/ether 32:68:e5:29:6a:b4 brd ff:ff:ff:ff:ff:ff link-netns cni-8deece12-a8d4-8394-86f4-8a6aaba25e23
36: vethff14a0b2@if2: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1300 qdisc noqueue master cni0 state UP mode DEFAULT group default 
    link/ether 92:a0:02:45:67:bc brd ff:ff:ff:ff:ff:ff link-netns cni-a52ff368-a13f-524f-9bd5-7dadb2b1a1c8
42: veth16a161bf@if2: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1300 qdisc noqueue master cni0 state UP mode DEFAULT group default 
    link/ether ca:3f:b0:2f:fe:1b brd ff:ff:ff:ff:ff:ff link-netns cni-c79c0359-2d16-7ac3-b332-ae4be1b40f5f
```


- 10.119.96.10 (044c82ef-1f4a-11f0-b290-7ea3ea26affc)
  - Worker w/ GPU
  - 云B
  - ip link show

```
1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN mode DEFAULT group default qlen 1000
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
2: eth0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1400 qdisc fq_codel state UP mode DEFAULT group default qlen 1000
    link/ether 76:81:4b:f0:0a:8f brd ff:ff:ff:ff:ff:ff
    altname enp1s0
3: flannel.1: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1350 qdisc noqueue state UNKNOWN mode DEFAULT group default 
    link/ether 36:99:cf:04:7c:b9 brd ff:ff:ff:ff:ff:ff
4: cni0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1300 qdisc noqueue state UP mode DEFAULT group default qlen 1000
    link/ether 4a:02:03:ea:6a:f5 brd ff:ff:ff:ff:ff:ff
6: veth3eb04351@if2: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1350 qdisc noqueue master cni0 state UP mode DEFAULT group default 
    link/ether a2:b4:73:61:ab:04 brd ff:ff:ff:ff:ff:ff link-netns cni-969dc6e8-d528-d31c-659f-9c751425b4dc
264: vethdba8614f@if2: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1300 qdisc noqueue master cni0 state UP mode DEFAULT group default 
    link/ether 3a:f9:6d:ea:de:23 brd ff:ff:ff:ff:ff:ff link-netns cni-a40f5379-f4b0-b336-c1df-a121095b5c6e
8: veth78142473@if2: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1350 qdisc noqueue master cni0 state UP mode DEFAULT group default 
    link/ether 0a:cb:b9:97:07:a6 brd ff:ff:ff:ff:ff:ff link-netns cni-4f5494eb-b428-9956-d6a3-c4f45b11055c
9: vethd5040eb2@if2: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1350 qdisc noqueue master cni0 state UP mode DEFAULT group default 
    link/ether ce:0b:22:79:a7:02 brd ff:ff:ff:ff:ff:ff link-netns cni-40989a2a-4ce6-91cc-1ec8-0e9b239f19d9
10: veth1a9e97f1@if2: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1350 qdisc noqueue master cni0 state UP mode DEFAULT group default 
    link/ether 22:64:53:52:cd:20 brd ff:ff:ff:ff:ff:ff link-netns cni-85905d6a-1aef-59fb-bba8-0e3e83cc2563
11: veth7a5d367a@if2: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1350 qdisc noqueue master cni0 state UP mode DEFAULT group default 
    link/ether 12:f7:9a:37:7f:44 brd ff:ff:ff:ff:ff:ff link-netns cni-6ddc5af7-719e-6c9e-da73-3dbf60529bbf
12: veth51b2b962@if2: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1350 qdisc noqueue master cni0 state UP mode DEFAULT group default 
    link/ether 1e:f0:e8:ef:fd:91 brd ff:ff:ff:ff:ff:ff link-netns cni-e5116f8e-1659-e974-d6bf-c48b859581f7
270: vethcf3c10ba@if2: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1300 qdisc noqueue master cni0 state UP mode DEFAULT group default 
    link/ether 8e:27:a6:25:3e:0e brd ff:ff:ff:ff:ff:ff link-netns cni-a9379729-4e8c-a2dd-43a2-f05aff283ba3
276: veth763a6248@if2: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1300 qdisc noqueue master cni0 state UP mode DEFAULT group default 
    link/ether ba:74:0b:a0:d4:93 brd ff:ff:ff:ff:ff:ff link-netns cni-54fb3d65-846f-dbb3-96a9-633624b7aad1
286: vethf903e60f@if2: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1300 qdisc noqueue master cni0 state UP mode DEFAULT group default 
    link/ether da:32:1d:f8:f0:73 brd ff:ff:ff:ff:ff:ff link-netns cni-a7056943-944a-edfb-5a8e-ecff5e676f35
180: veth84d63c69@if2: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1300 qdisc noqueue master cni0 state UP mode DEFAULT group default 
    link/ether 02:ed:81:4b:33:9d brd ff:ff:ff:ff:ff:ff link-netns cni-7898341f-1e04-5a3d-47cd-77434b1a11c3
187: vethda57cc74@if2: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1300 qdisc noqueue master cni0 state UP mode DEFAULT group default 
    link/ether a2:18:4b:4d:3a:22 brd ff:ff:ff:ff:ff:ff link-netns cni-c53ca540-ab7a-e09b-ee60-51ed1af42be3
193: veth2556937a@if2: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1300 qdisc noqueue master cni0 state UP mode DEFAULT group default 
    link/ether 2a:52:2f:96:7e:69 brd ff:ff:ff:ff:ff:ff link-netns cni-949a729c-c6a7-fc06-a2a2-e7072677d611
220: vethacefa480@if2: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1300 qdisc noqueue master cni0 state UP mode DEFAULT group default 
    link/ether 02:96:45:17:02:77 brd ff:ff:ff:ff:ff:ff link-netns cni-cbd97913-467e-a607-4d0e-8ea175fa6539
221: vetha680e6b8@if2: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1300 qdisc noqueue master cni0 state UP mode DEFAULT group default 
    link/ether 42:25:be:dc:f9:8c brd ff:ff:ff:ff:ff:ff link-netns cni-5f66f9cd-a8e3-d1e0-0923-d267db6ebb40
224: veth46ca3d39@if2: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1300 qdisc noqueue master cni0 state UP mode DEFAULT group default 
    link/ether 32:8a:fe:49:93:78 brd ff:ff:ff:ff:ff:ff link-netns cni-5751b6df-13fd-bc80-17bd-8ed81aefa393
230: veth26d073f0@if2: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1300 qdisc noqueue master cni0 state UP mode DEFAULT group default 
    link/ether 1e:90:39:60:aa:02 brd ff:ff:ff:ff:ff:ff link-netns cni-f45cc8f4-1036-c867-4384-151b33d20444
232: vethc9b41e0a@if2: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1300 qdisc noqueue master cni0 state UP mode DEFAULT group default 
    link/ether 32:4c:bb:93:09:9e brd ff:ff:ff:ff:ff:ff link-netns cni-b9cc67f4-2931-525c-959f-c96041f85743
239: veth7e7fd433@if2: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1300 qdisc noqueue master cni0 state UP mode DEFAULT group default 
    link/ether 7e:78:fe:8f:00:a9 brd ff:ff:ff:ff:ff:ff link-netns cni-4e0be1a5-eb0a-ab07-0622-59f447975cc3
```

云 A 和云 B 之间是通过 SD-WAN 方案来打通的。机器之间可以相互 ping 通。

集群使用 `ingress-nginx` 来作为 ingressClass。
集群使用 `flannel` 来作为 pod 通信的网络插件。

集群上部署业务服务：
- mos-gateway-api
  - Host 为 mos-dev.example.tech 
  - 处理 /api/v1/ 相关接口

# 2. 问题

## 定位过程

Dev 环境部署完成后，在浏览器可以正常访问前端。新建任务请求一直失败，通过 console 查看，是网络请求超时，具体请求的 `curl` 命令为：

```
curl --request POST \
  --url https://mos-dev.example.tech/api/v1/projects/104781033694208/tasks \
  --header 'accept: application/json' \
  --header 'accept-language: en-US,en;q=0.9,zh-CN;q=0.8,zh;q=0.7' \
  --header 'content-type: application/json' \
  --header 'origin: https://mos-dev.example.tech' \
  --header 'priority: u=1, i' \
  --header 'referer: https://mos-dev.example.tech/' \
  --header 'sec-ch-ua: "Google Chrome";v="137", "Chromium";v="137", "Not/A)Brand";v="24"' \
  --header 'sec-ch-ua-mobile: ?0' \
  --header 'sec-ch-ua-platform: "macOS"' \
  --header 'sec-fetch-dest: empty' \
  --header 'sec-fetch-mode: cors' \
  --header 'sec-fetch-site: same-origin' \
  --header 'user-agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/137.0.0.0 Safari/537.36' \
  --header 'x-requested-with: XMLHttpRequest' \
  --data '{
  "name": "testdev",
  "description": "",
  "type": "ligandmpnn",
  "workId": "0",
  "payload": "{\"structure_path\":\"user_objects/123/1001/166430451258368_5xh3.pdb\",\"__structure_pathFileId\":\"166430451258368\",\"__structure_pathFilename\":\"5xh3.pdb\",\"tos_file\":[\"structure_path\"],\"task_name\":\"testdev\"}"
}'
```

尝试去掉请求中的所有 `header` ，发现请求成功。

### 1. header 问题？

尝试逐步去掉 header，发现 `sec-*` 和 `user-agent x-requested-with` 保留任意一个都可以请求成功。
通过 `kubectl logs -f -n ingress-nginx -l app.kubernetes.io/name=ingress-nginx` 查看访问流量。
发现超时流量在 ingress-nginx 中有访问日志，但通过查看 `mos-gateway-api` 服务日志，没发现有访问流量。
怀疑是不是 `mos-gateway-api` 服务将流量丢弃了，或者没办法处理。在服务内部加上日志打印中间件，在服务入口就打印日志。尝试后没有发现预期日志。
经过和 LLM 讨论后，怀疑是不是 `cors` 配置有问题，注释掉相关的代码。上线后请求仍然不成功，没有预期日志。

### 2. 网络问题？
随便去掉几个 header，发现就算 `sec-*` 和 `user-agent x-requested-with` 头都在，请求仍然可以调通，但全部都有的时候调不通。发现这些尝试只和请求体大小相关。
尝试去掉所有的 header，在 POST body 中添加随机字符串，当请求体变大时，请求失败。

> [!IMPORTANT]
> 初步结论：请求的问题和请求体大小有关，和 header 没关系。


在 ingress-nginx 的 controller deployment 的 `args` 中加上 `-v=4` 的参数，打印更多日志。重复请求，发现有些请求可以成功，有些失败。
跟踪请求，发现：

- 10.245.1.1 失败：pod 是 `ingress-nginx-controller-84654bf89f-g2hpm`，在 `dev-worker` 节点。在云 A 上；
- 10.245.1.0 成功：pod 是 `ingress-nginx-controller-84654bf89f-mlqhd`，在 `044c82ef-1f4a-11f0-b290-7ea3ea26affc` 节点。在云 B 上；

而 `mos-gateway-api` 服务在更新后，被调度到 `044c` 节点上。怀疑是不是云 A 到云 B 节点的 pod 之间的网络问题？

登录到 `192.168.1.131` 机器上，用 `CLUSTER-IP` 来请求 `mos-gateway-api` 服务。请求失败，表现和之前一样，一直阻塞住。

```
# kubectl get svc -n mos
NAME                   TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)                                                                            AGE
mos-gateway-api        ClusterIP   10.102.115.141   <none>        8888/TCP                                                                           4d8h

# curl --request POST \
  --url http://10.102.115.141:8888/v1/projects/104781033694208/tasks \
  --header 'content-type: application/json' \
  --data '{
  "name": "testdev",
  "description": "",
  "type": "ligandmpnnxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxligandmpnnxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxligandmpnnxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxligandmpnnxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxligandmpnnxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxligandmpnnxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxaaaaaaaaabbbbccccddddeeeeffffgggghhhhiiiijjjjkkkkllllmmmmligandmpnnxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxligandmpnnxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxligandmpnnxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxligandmpnnxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxligandmpnnxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxligandmpnnxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxligandmpnnxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxligandmpnnxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxligandmpnnxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxligandmpnnxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxaaaaaaaaabbbbccccddddeeeeffffgggghhhhiiiijjjjkkkkllllmmmmligandmpnnxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxligandmpnnxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxligandmpnnxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxligandmpnnxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxligandmpnnxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxligandmpnnxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxligandmpnnxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxligandmpnnxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxligandmpnnxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxligandmpnnxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxaaaaaaaaabbbbccccddddeeeeffffgggghhhhiiiijjjjkkkkllllmmmmligandmpnnxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxligandmpnnxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxligandmpnnxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxligandmpnnxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxligandmpnnxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxligandmpnnxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxligandmpnnxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxligandmpnnxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxligandmpnnxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxligandmpnnxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxaaaaaaaaabbbbccccddddeeeeffffgggghhhhiiiijjjjkkkkllllmmmmligandmpnnxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxligandmpnnxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxligandmpnnxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxligandmpnnxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx",
  "workId": "0",
  "payload": "{\"structure_path\":\"user_objects/123/1001/166430451258368_5xh3.pdb\",\"__structure_pathFileId\":\"166430451258368\",\"__structure_pathFilename\":\"5xh3.pdb\",\"tos_file\":[\"structure_path\"],\"task_name\":\"testdev\"}"
}' -v

Note: Unnecessary use of -X or --request, POST is already inferred.
*   Trying 10.102.115.141:8888...
* Connected to 10.102.115.141 (10.102.115.141) port 8888 (#0)
> POST /v1/projects/104781033694208/tasks HTTP/1.1
> Host: 10.102.115.141:8888
> User-Agent: curl/7.88.1
> Accept: */*
> content-type: application/json
> Content-Length: 1616
> 
```

> [!IMPORTANT]
> 问题从“前端请求服务超时”简化成“从集群内部 host 机器请求集群内部服务超时”。

通过 `tcpdump -i any port 8888 -w port_8888_capture.pcap` 抓包，抓包结果：[port_8888_capture.pcap](https://github.com/HuihuangZhang/blog/blob/master/notes/issue3/port_8888_capture.pcap)


> 请求的包都是通过 flannel.1 网卡来发送出去的。这是符合集群流量的预期的，经过 flannel.1 后，配置的是 vxlan 网络，会被封装一层，到 8472 端口出去。但已经不可读了。

结合机器的 `ip link show` 结果通过 LLM 分析后

### 具体 LLM 分析参考

Let's analyze the provided `tcpdump` output and `ip link show` information to understand why your HTTP service is blocking with larger body sizes.

---
#### Understanding the Network Setup
From your `ip link show` output, we can deduce the following:
- Host Network Interfaces:
  - Both `hostA` and `hostB` have an `eth0` interface with an **MTU of 1400 bytes**.
- Kubernetes Network Interfaces (Flannel):
  - `flannel.1`: This is the overlay network interface used by Flannel for inter-node pod communication. Both `hostA` and `hostB` show `flannel.1` with an MTU of 1350 bytes. This is expected, as Flannel typically encapsulates traffic (adding an overhead for its VXLAN/UDP header), which reduces the effective MTU for the encapsulated traffic.
  - `cni0`: This is the bridge interface created by the CNI plugin (Flannel in this case) on each node. It connects the pods to the Flannel network. Both hostA and hostB show cni0 **with an MTU of 1300 bytes**.
  - `veth` interfaces: These are virtual ethernet pairs connecting individual pods to the cni0 bridge. On hostA, you have veth interfaces with an MTU of 1300 bytes (e.g., `veth16a161bf@if2`). On hostB, you have a mix of veth interfaces, some with 1350 bytes and some with 1300 bytes. The relevant one for your curl command (from hostA to a pod on hostB) appears to be on hostA (source of the curl). The veth interface MTU should generally align with `cni0`'s MTU or be slightly higher if it's the host-side of the veth pair.

---
#### Analyzing the tcpdump Output
The tcpdump output clearly shows a Maximum Segment Size (MSS) issue, leading to retransmissions and ultimately connection blocking for larger data transfers. Let's break down the key lines:
1. Initial Connection (HostA Initiates):
  - `15:50:42.501515 flannel.1 Out IP 10.245.1.0.58490 > 10.245.2.26.8888: Flags [S], seq 498438729, win 65280, options [mss 1360,sackOK,TS val 956633286 ecr 0,nop,wscale 7], length 0`
    - HostA (source IP 10.245.1.0) sends a SYN packet with an advertised MSS of 1360 bytes. This MSS is based on the MTU of its flannel.1 interface (1350) minus the IP and TCP header size (typically 20 bytes each, so 40 bytes total). 1350−40=1310. Correction: It's MSS + TCP/IP header, so the MSS should be 1350 - 20 (IP) - 20 (TCP) = 1310. The MSS 1360 is higher than expected for the flannel.1 MTU of 1350. This suggests the client's (curl's) TCP stack is possibly negotiating a higher MSS initially, or it's seeing the MTU of the underlying eth0 before Flannel encapsulation. Given the flannel.1 interface is used, the 1350 MTU for flannel is key.
  - `15:50:42.535244 flannel.1 In IP 10.245.2.26.8888 > 10.245.1.0.58490: Flags [S.], seq 1541717263, ack 498438730, win 64896, options [mss 1260,sackOK,TS val 1105819668 ecr 956633286,nop,wscale 7], length 0`
    - HostB (destination IP 10.245.2.26) responds with a SYN-ACK, advertising an MSS of 1260 bytes. This is a crucial piece of information. The pod on HostB is likely setting its MSS based on its cni0 MTU (1300) minus IP/TCP headers (1300−40=1260). This MSS of 1260 is what the server-side believes it can send.
2. Data Transfer and Retransmissions:
  - `15:50:42.535312 flannel.1 Out IP 10.245.1.0.58490 > 10.245.2.26.8888: Flags [P.], seq 1:1788, ack 1, win 510, options [nop,nop,TS val 956633320 ecr 1105819668], length 1787`
    - HostA sends a packet with a length of 1787 bytes. This is significantly larger than the advertised MSS of 1260 from HostB and even larger than the flannel.1 MTU of 1350 on HostA itself. This packet is clearly too large for the network path.
  - `15:50:42.568308 flannel.1 In IP 10.245.2.26.8888 > 10.245.1.0.58490: Flags [.], ack 1, win 507, options [nop,nop,TS val 1105819701 ecr 956633320,nop,nop,sack 1 {1249:1788}], length 0`
    - HostB acknowledges up to sequence 1 but also sends a SACK (Selective Acknowledgment) indicating that it received the segment from 1249 to 1788 (meaning the initial part of the large segment was lost or dropped). This is a strong indicator of packet fragmentation or dropping due to MTU mismatch.
  - `15:50:42.584644 flannel.1 Out IP 10.245.1.0.58490 > 10.245.2.26.8888: Flags [.], seq 1:1249, ack 1, win 510, options [nop,nop,TS val 956633370 ecr 1105819701], length 1248`
    - HostA now retransmits the initial part of the data, but this time with a length of 1248 bytes. This looks like HostA has adapted its sending size, likely due to Path MTU Discovery (PMTUD) or some other mechanism detecting the smaller effective MTU.
  - Subsequent lines starting from 15:50:42.828648 show repeated retransmissions of the same 1248-byte segment from HostA, without further acknowledgments from HostB for the remaining data. This indicates that even the 1248-byte packets might be encountering issues or that the initial large packet broke the connection's flow. The increasing time between retransmissions (0.2s, 0.5s, 1s, 2s, 4s, etc.) is typical of TCP's exponential backoff for retransmissions.

---
#### The Root Cause: MTU Mismatch and PMTUD Failure
The problem is almost certainly related to Maximum Transmission Unit (MTU) mismatch and potential Path MTU Discovery (PMTUD) failure.
Here's why:
- Initial Large Packet: Your curl command, when requesting a large body, causes the sending side (HostA's pod) to attempt to send a TCP segment larger than what the network path can handle. 
  - HostA's flannel.1 has an MTU of 1350.
  - HostB's cni0 has an MTU of 1300, and the service pod likely has a veth pair connected to cni0 also with a 1300 MTU.
  - The tcpdump shows HostA sending a packet with a length of 1787 bytes, which is much larger than any of the observed interface MTUs within the Kubernetes overlay network. This packet is either being fragmented, or more likely, being dropped along the path due to the "Don't Fragment" (DF) bit being set on the IP header (which is common for PMTUD to work).
- MSS Negotiation Discrepancy:
  - HostA (client) initially advertised an MSS of 1360.
  - HostB (server pod) advertised an MSS of 1260. This 1260 MSS is critical. It implies that the TCP stack on HostB's pod is telling HostA that the largest segment it can receive without fragmentation is 1260 bytes (payload).
- PMTUD Issues: When HostA sends the 1787-byte packet, it's exceeding the effective MTU of the path. If PMTUD were working perfectly, an ICMP "Fragmentation Needed" (Type 3, Code 4) message would be sent back to HostA, indicating the correct MTU of the path. However, these ICMP messages are often blocked by firewalls (either on the hosts, cloud network, or intermediate devices), leading to PMTUD blackholing. When PMTUD blackholing occurs, the sender never receives the ICMP message and continues to send packets that are too large, leading to retransmissions and connection stalls, which is precisely what you are observing.
- Flannel Overlay Overhead: Flannel uses VXLAN encapsulation, which adds its own header overhead (typically 50 bytes for UDP and VXLAN headers). 
  - If eth0 MTU is 1400, then flannel.1 MTU should be 1400−50=1350. This matches your flannel.1 MTU.
  - The pod's effective MTU is further reduced. If flannel.1 has an MTU of 1350, then the cni0 bridge and the veth interfaces for the pods would typically have an MTU of 1350−bridge/veth overhead (if any)−IP/TCP headers. The 1300 MTU for cni0 and veth pairs seems consistent with this, suggesting an additional 50 bytes of overhead between flannel.1 and cni0 (e.g., for the bridge itself or for internal Kubernetes networking mechanisms).
The server's reported MSS of 1260 (which implies an effective path MTU of 1260+40=1300) seems to be the most accurate reflection of the actual usable MTU within the pod network. The client sending segments of 1787 bytes is the direct cause of the blocking.

---
#### Why Small Body Sizes Work
For small body sizes, the HTTP response data fits within a single TCP segment that is smaller than the smallest effective MTU (1260 bytes payload). Therefore, no fragmentation or PMTUD issues occur, and the transfer completes successfully.

---
#### Solutions
The core issue is that your client (on HostA, specifically the curl process within a pod, or the host's kernel if curl is on the host) is sending packets larger than the effective MTU of the Kubernetes network path, particularly when crossing the Flannel overlay.
Here are the primary solutions:
1. Adjust Flannel's MTU Configuration (Recommended): This is the most common and robust solution. You need to ensure that Flannel's MTU is correctly configured across your cluster, considering the underlying eth0 MTU and the VXLAN overhead.
  - The eth0 MTU is 1400.
  - Flannel's default overhead for VXLAN is 50 bytes.
  - Therefore, Flannel's MTU should ideally be 1400−50=1350. Your current flannel.1 MTU of 1350 is correct.
  - However, your cni0 and veth interfaces inside the pods have an MTU of 1300, which suggests an additional 50 bytes of overhead somewhere in the pod network configuration (e.g., related to IP in IP encapsulation or specific CNI configurations). The pod on HostB advertises an MSS of 1260, which implies an MTU of 1300.
  - Action: Explicitly set the MTU for Flannel in its configuration (e.g., in the Flannel CNI manifest). You want to ensure that the pods and the cni0 bridge negotiate an MTU that works end-to-end. You can try setting --mtu=1350 for the Flannel daemonset (if you're using kube-flannel.yml, look for the args section for the kube-flannel container). If setting it to 1350 doesn't entirely resolve it, try setting it to 1300 to match the lowest observed cni0 MTU across your nodes, or even lower (e.g., 1250) if the problem persists, to give some buffer.
  - Why 1300 might be the right target: If the smallest MTU for cni0 or the pod veth interfaces is 1300, then the TCP MSS should be 1260. The initial packet size of 1787 bytes sent from the client (HostA pod) is too large for this path. By forcing Flannel and CNI to use a lower, consistent MTU, you ensure all packets fit.
2. Enable MSS Clamping (or ensure it's working): MSS clamping ensures that the TCP MSS is adjusted to fit the MTU of the outgoing interface. For Kubernetes with Flannel, this is often done on the flannel.1 interface using iptables rules.
  - You should see an iptables rule like MASQUERADE --to-ports 1024-65535 -m comment --comment "flannel for --mtu" or similar that also performs TCPMSS --set-mss.
  - Action: Verify that MSS clamping is active on flannel.1 on both nodes. You can check iptables -t mangle -L or iptables -t filter -L to see if there are rules specifically setting the TCP MSS. The rule would look something like: 
  - Bash
`iptables -t mangle -A POSTROUTING -o flannel.1 -p tcp -m tcp --tcp-flags SYN,RST SYN -j TCPMSS --set-mss 1300`
  -  (Replace 1300 with your target MTU - 40 bytes for TCP/IP headers).
  - The fact that the client (HostA) sent a segment of 1787 bytes even after HostB advertised an MSS of 1260 suggests that either the client's TCP stack isn't respecting the advertised MSS, or there's an issue with how the curl command itself is behaving, or a PMTUD failure is preventing the client from discovering the actual path MTU. MSS clamping on the egress interface (flannel.1) is designed to prevent this.
3. Check for ICMP Blocking:
  - If PMTUD is failing due to ICMP "Fragmentation Needed" packets being dropped, this would explain the retransmissions and blocking.
  - Action: Ensure that ICMP Type 3, Code 4 packets are allowed in your firewalls on both hostA and hostB (and any network devices in between). This is crucial for PMTUD to work correctly.

---
#### Recommended Steps to Troubleshoot and Fix:
1. Verify Flannel MTU Configuration:
  - Edit your Flannel DaemonSet YAML (kube-flannel.yml or similar) to explicitly set the MTU. Based on your cni0 and veth MTUs, 1300 seems like a safe bet.
  - Find the kube-flannel-ds DaemonSet and look for the args section under the kube-flannel container. Add or modify the --mtu argument: 
  - YAML
```
containers:
- name: kube-flannel
  image: ...
  command:
    - /opt/bin/flanneld
  args:
    - --ip-masq
    - --kube-subnet-mgr
    - --iface=eth0
    - --mtu=1300  # <--- Add or modify this line
```
  - Apply the changes: kubectl apply -f kube-flannel.yml
  - Restart Flannel pods (if they don't restart automatically).
2. Verify MSS Clamping:
  - After adjusting Flannel MTU, check iptables -t mangle -L on both HostA and HostB. Look for a rule that clamps the MSS for TCP packets going out on the flannel.1 interface.
  - Example rule (might vary slightly depending on Flannel version): 
`-A POSTROUTING -o flannel.1 -p tcp -m tcp --tcp-flags SYN,RST SYN -j TCPMSS --set-mss 1260`
  -  (This would set the MSS to 1260, which corresponds to an MTU of 1300).
3. Test ICMP Reachability (Optional but good for understanding):
  - From a pod on hostA, try ping -M do -s 1272 <POD-IP-ON-HOSTB> where 1272 is 1300−28 (ICMP header and IP header). If you get "Frag needed and DF set" errors, it confirms the MTU constraint. If it hangs, ICMP might be blocked.
By ensuring a consistent and correctly calculated MTU across your Kubernetes network interfaces and verifying that MSS clamping is in place, you should resolve the issue of large HTTP body transfers blocking. The key is to make sure that no packet sent by any part of the Kubernetes network exceeds the smallest effective MTU on the path.

## 结论

结论是 MTU 不匹配导致的数据包一直重传。

# 3. 解决办法

修改 `flannel.1` 配置，统一 `vethxxx` 相关网卡的 MTU。修改成 1300，让网络插件自己更新 `vethxxx` 的 MTU 大小。

`kubectl edit cm kube-flannel-cfg -n kube-flannel`

```
# ...
net-conf.json:
----
{
  "Network": "10.245.0.0/16",
  "EnableNFTables": false,
  "Backend": {
    "Type": "vxlan",
    "MTU": 1300
  }
}
```

运行 `kubectl rollout restart daemonset kube-flannel-ds -n kube-flannel` 更新 flannel 相关服务。

运行 `kubectl rollout restart deploy -n mos` 重启 mos 中的所有服务。

> 最好更新所有 namespace 下的所有服务，将 pod 内的 eth0 的大小都修改成预期值。

修改完重启后，再次请求 `curl` 命令，可以成功执行。